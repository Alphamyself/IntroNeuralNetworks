{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Building Models On Real Data with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangle the Boston House Price Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677082</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677082   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT        MEDV  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For info on the dataset: https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html\n",
    "dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\"\n",
    "colnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = pd.read_csv(dataset_url, delim_whitespace=True, names=colnames)\n",
    "\n",
    "# Get a gist of your data\n",
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
       "       'PTRATIO', 'B', 'LSTAT', 'MEDV', 'CHAS_0', 'CHAS_1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that there is one catagorical variable (CHAS)\n",
    "# Apply one hot encoding (also called dummy variables)\n",
    "dataframe = pd.get_dummies(dataframe, columns=[\"CHAS\"])\n",
    "\n",
    "# Print the new columns\n",
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the target values as the median value of houses\n",
    "# Set all other variables as the explanatory variables\n",
    "# Use the zscore to standardize the numerical variables\n",
    "targets = zscore(dataframe.MEDV)\n",
    "predictors = dataframe.drop(\"MEDV\", axis=1).as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Your First (Regression) Model with Keras!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of columns: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Set up the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a hidden layer\n",
    "model.add(Dense(n_cols, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "model.add(Dense(n_cols//2, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1, input_shape=(n_cols,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the adam optimizer and mean_squared_error loss function\n",
    "model.compile(optimizer=\"sgd\", loss='mse', metrics=[\"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan                 \n",
      "Epoch 2/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 3/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 4/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 5/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 6/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 7/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 8/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 9/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 10/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 11/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 12/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 13/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 14/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 15/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 16/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 17/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 18/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 19/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan      ETA: 0s - loss: nan - mean_squared_error: n\n",
      "Epoch 20/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 21/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 22/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 23/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 24/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 25/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 26/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 27/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 28/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 29/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 30/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 31/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 32/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 33/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 34/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 35/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 36/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 37/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 38/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 39/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 40/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 41/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 42/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan      ETA: 0s - loss: nan - mean_squared_error: n\n",
      "Epoch 43/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 44/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 45/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 46/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 47/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 48/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 49/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 50/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 51/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 52/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 53/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 54/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 55/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 56/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 57/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 58/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 59/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 60/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 61/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 62/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 63/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 64/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 65/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 66/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 67/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 68/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 69/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 70/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 71/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 72/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 73/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 74/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 75/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan      ETA: 0s - loss: nan - mean_squared_error: n\n",
      "Epoch 76/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 77/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 78/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 79/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 81/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 82/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 83/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 84/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 85/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 86/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 87/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 88/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 89/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 90/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 91/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 92/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 93/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 94/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 95/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 96/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 97/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 98/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 99/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 100/100\n",
      "506/506 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(predictors, targets, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x119bcaa90>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFDYZsBaypoYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119b75ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well can the model aproximate simple functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build Your First (Classification) Model with Keras!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets\n",
      " [[ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " ..., \n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]]\n",
      "Targets and predictors from titantic data set loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Do all the data wrangling on the backend\n",
    "%run data_wrangling_titantic.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Set up the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a hidden layer\n",
    "model.add(Dense(n_cols, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "# Add the output layer\n",
    "# Notice a softmax activation on the output\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: categorical_crossentropy\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "# Verify that model contains information from compiling\n",
    "print(\"Loss function: \" + model.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 623 samples, validate on 268 samples\n",
      "Epoch 1/10\n",
      "623/623 [==============================] - 0s - loss: 1.2644 - acc: 0.6051 - val_loss: 0.5432 - val_acc: 0.7463\n",
      "Epoch 2/10\n",
      "623/623 [==============================] - 0s - loss: 0.6695 - acc: 0.6613 - val_loss: 0.5321 - val_acc: 0.7463\n",
      "Epoch 3/10\n",
      "623/623 [==============================] - 0s - loss: 0.6184 - acc: 0.6677 - val_loss: 0.5565 - val_acc: 0.7052\n",
      "Epoch 4/10\n",
      "623/623 [==============================] - 0s - loss: 0.5656 - acc: 0.7159 - val_loss: 0.4857 - val_acc: 0.7575\n",
      "Epoch 5/10\n",
      "623/623 [==============================] - 0s - loss: 0.5435 - acc: 0.7255 - val_loss: 0.4982 - val_acc: 0.7575\n",
      "Epoch 6/10\n",
      "623/623 [==============================] - 0s - loss: 0.5544 - acc: 0.7496 - val_loss: 0.4944 - val_acc: 0.7500\n",
      "Epoch 7/10\n",
      "623/623 [==============================] - 0s - loss: 0.5527 - acc: 0.7448 - val_loss: 0.4497 - val_acc: 0.7985\n",
      "Epoch 8/10\n",
      "623/623 [==============================] - 0s - loss: 0.4995 - acc: 0.7785 - val_loss: 0.4763 - val_acc: 0.7836\n",
      "Epoch 9/10\n",
      "623/623 [==============================] - 0s - loss: 0.4902 - acc: 0.7865 - val_loss: 0.4325 - val_acc: 0.8022\n",
      "Epoch 10/10\n",
      "623/623 [==============================] - 0s - loss: 0.4812 - acc: 0.7881 - val_loss: 0.4440 - val_acc: 0.7948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11a593828>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "model.fit(predictors, targets, epochs=10, batch_size=5, validation_split=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 623 samples, validate on 268 samples\n",
      "Epoch 1/100\n",
      "623/623 [==============================] - 0s - loss: 0.4841 - acc: 0.7913 - val_loss: 0.6415 - val_acc: 0.7500\n",
      "Epoch 2/100\n",
      "623/623 [==============================] - 0s - loss: 0.4730 - acc: 0.7913 - val_loss: 0.4345 - val_acc: 0.7985\n",
      "Epoch 3/100\n",
      "623/623 [==============================] - 0s - loss: 0.4694 - acc: 0.8058 - val_loss: 0.4297 - val_acc: 0.7948\n",
      "Epoch 4/100\n",
      "623/623 [==============================] - 0s - loss: 0.4857 - acc: 0.7897 - val_loss: 0.4197 - val_acc: 0.8097\n",
      "Epoch 5/100\n",
      "623/623 [==============================] - 0s - loss: 0.4618 - acc: 0.8170 - val_loss: 0.4591 - val_acc: 0.7985\n",
      "Epoch 6/100\n",
      "623/623 [==============================] - 0s - loss: 0.4609 - acc: 0.8058 - val_loss: 0.4192 - val_acc: 0.7948\n",
      "Epoch 7/100\n",
      "623/623 [==============================] - 0s - loss: 0.4661 - acc: 0.7913 - val_loss: 0.5118 - val_acc: 0.7948\n",
      "Epoch 8/100\n",
      "623/623 [==============================] - 0s - loss: 0.4618 - acc: 0.7897 - val_loss: 0.4287 - val_acc: 0.8060\n",
      "Epoch 9/100\n",
      "623/623 [==============================] - 0s - loss: 0.4698 - acc: 0.7994 - val_loss: 0.4325 - val_acc: 0.8097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x119d81b00>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss',  patience=2)\n",
    "\n",
    "model.fit(x=predictors, y=targets, epochs=100, batch_size=5, validation_split=.3, callbacks=[early_stopping])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
