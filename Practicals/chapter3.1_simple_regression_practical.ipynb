{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground: Regression with One Explanatory Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "  - regularization\n",
    "  - catching irregularities\n",
    "  - noise in the data  \n",
    "  \n",
    "Building\n",
    "  - layers\n",
    "  - nodes\n",
    "  - activation functions  \n",
    "  \n",
    "Compiling\n",
    "  - learning rate\n",
    "  - loss functions  \n",
    "  \n",
    "Training\n",
    "  - epochs\n",
    "  - batch size\n",
    "  - optimizers\n",
    "  - overfitting\n",
    "  - ? dropout\n",
    "  - ? early stop  \n",
    "  \n",
    "Testing\n",
    "  - ? validation split, validation loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: in the following, the data arrays get transposed a lot. Don't worry about it.\n",
    "# ... just different functions require that the data come in different shapes.\n",
    "\n",
    "# generate some fake input_data (n data points in range 1 to n with uniform distribution)\n",
    "n = ___\n",
    "# set how random you want your data to be in the x direction (try between 0-5)\n",
    "x_rand_scale = ____\n",
    "input_data = np.array(range(n)) + np.random.normal(loc=0, scale=x_rand_scale, size=n)\n",
    "\n",
    "# normalize the input_data by transforming each value to its respective zscore\n",
    "# neural networks work much better with normalized values\n",
    "# fill in the blank with 'zscore'\n",
    "norm_data = scipy.stats.____(input_data.transpose())\n",
    "\n",
    "# The following are some predefined distributions for your data set\n",
    "linear = norm_data\n",
    "parabolic = norm_data**2\n",
    "sin = np.sin(2*norm_data)\n",
    "discontinuous = np.array([0 if i > -1  and i < 1 else i*-4 for i in norm_data])\n",
    "\n",
    "# Set targets to one of the previously defined distributions or write your own\n",
    "targets = _________\n",
    "\n",
    "# Add some noise (as all data has). Pick noise between 0-1\n",
    "noise = ____\n",
    "targets = targets.transpose() + np.random.normal(loc=0.0, scale=noise, size=n)\n",
    "\n",
    "# Reshape your data to feed into the model\n",
    "norm_data = np.array([[i] for i in norm_data])\n",
    "targets = np.array([[i] for i in targets])\n",
    "\n",
    "# Plot the data in orange dots\n",
    "plt.plot(norm_data, targets, 'ro', markersize=2, color=\"orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return a Built and Compiled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a built and compiled model\n",
    "# n_cols: number of explanatory features you're planning to feed into the model\n",
    "# inside this function, you can adjust the number of layers, \n",
    "# ... nodes per layer, activation functions, optimizer type,\n",
    "def get_regression_model(num_features):\n",
    "    # Set up a 'Sequential()' (feed-forward) model\n",
    "    model = ___________\n",
    "\n",
    "    # add a hidden layer. First layer must declare input_shape being the number of features you're feeding into it.\n",
    "    # try different number of nodes (units). Try different activation functions (\"relu\", \"elu\", \"tanh\", sigmoid\"). \n",
    "    # if you remove the activation parameter completely, there will be no activation function (ie identity function)\n",
    "    model.add(Dense(units=___, activation=____, input_shape=(num_features,)))\n",
    "\n",
    "    # What about adding another hidden layer?\n",
    "\n",
    "    # Add the output layer. Only 1 node for regression models in the output layer and no activation function.\n",
    "    model.add(Dense(units=__))\n",
    "\n",
    "    # Try setting a learning rate (lr) for the Stochastic Gradient Decent (SGD) optimizer\n",
    "    # and set the loss function, \"loss\" to mean_squared_error \"mse\".\n",
    "    # ... try other optimizers like \"adam\" by setting optimizer=\"adam\"\n",
    "    # ... try other loss functions (https://keras.io/losses/)\n",
    "    SGD = keras.optimizers.sgd(lr=__)\n",
    "    model.compile(optimizer=SGD, loss=___, metrics=[\"mse\"])\n",
    "\n",
    "    # return the model built and compiled\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Keras Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a new regression model: model\n",
    "# only one feature is being fed into this model\n",
    "model = get_regression_model(num_features=1)\n",
    "\n",
    "# start a log of the mean squared error: log\n",
    "log = {\"mse\":[]}\n",
    "\n",
    "# set the number of rounds to train (each round will be multiple epochs)\n",
    "training_iterations = 10\n",
    "\n",
    "# fit your model once to see what the model guesses (in green) after one try through your data\n",
    "# set the epoch to 1 meaning only go through the data set once\n",
    "history = model.fit(x=norm_data, y=targets, epochs=__, verbose=0)\n",
    "# attach the history about the mean_squared_error to the log\n",
    "log[\"mse\"].extend(history.history['mean_squared_error'])\n",
    "# try to predict the data set with the model\n",
    "predictions = model.predict(norm_data)\n",
    "# add a plot of those predictions to the figure\n",
    "plt.plot(norm_data, predictions, alpha=.5, color=\"green\")\n",
    "# add a legend entry for this first preditcion\n",
    "plt.legend(['First Guess'], loc='best')\n",
    "\n",
    "\n",
    "# then run through your data 'training_iterations' more times\n",
    "# each iteration will run your model through 5 more epochs (becuase thats what epochs is set to)\n",
    "for i in range(training_iterations):\n",
    "    # at each iteration, record the history of how your models performance\n",
    "    # try adjusting the batch size to find (suggested 1-20)\n",
    "    history = model.fit(x=______, y=______, epochs=5, batch_size=___, verbose=0)\n",
    "    # save that history to a log for plotting\n",
    "    log[\"mse\"].extend(history.history['mean_squared_error'])\n",
    "    # have the model make predictions off of your normalized input data (like above)\n",
    "    predictions = model.predict(______)\n",
    "    # alpha is just a coefficient between 0-1 that will be used to set the color and opacity of the plot\n",
    "    alpha = (i+1)/training_iterations\n",
    "    # add a plot to the figure of what your machine guessed for your input data after this iteration\n",
    "    # \"earlier\" guesses will be more faded while \"later\" guesses will be more vivid\n",
    "    plt.plot(norm_data, predictions, alpha=alpha, color=(alpha, 0.2, 0.5))\n",
    "\n",
    "# add the plot of the initial input data to the figure\n",
    "plt.plot(norm_data, targets, 'ro', markersize=2, color=\"orange\")\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now access how your model did on the training data after each iteration by graphing the 'Mean Squared Error'\n",
    "first_loss = round(log[\"mse\"][0], 5)\n",
    "final_loss = round(log[\"mse\"][-1], 5)\n",
    "plt.plot(log[\"mse\"])\n",
    "plt.annotate(first_loss,(0,log[\"mse\"][0]))\n",
    "plt.annotate(final_loss,(n,log[\"mse\"][-1]))\n",
    "plt.legend(['Mean Squared Error'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
